# ОТЧЕТ ПО ЛАБОРАТОРНОЙ РАБОТЕ

**Дисциплина:** Искусственный интеллект  
**Тема:** Деревья решений. Алгоритмы ID3, C4.5, CART, CHAID  
**Семинар:** 13  
**Предметная область:** Выбор заказчика  

---

## 1. ЦЕЛИ И ЗАДАЧИ

### 1.1. Цель работы

Изучить и реализовать алгоритмы построения деревьев решений для задач классификации, исследовать различия между алгоритмами ID3, C4.5, CART и CHAID, применить полученные знания на практической задаче выбора заказчика.

### 1.2. Задачи

1. **Теоретическая подготовка:**
   - Изучить теорию деревьев решений
   - Разобраться в математических основах алгоритмов ID3, C4.5, CART и CHAID
   - Понять концепции энтропии, прироста информации, индекса Джини и критерия хи-квадрат

2. **Практическая реализация:**
   - Создать обучающую выборку из 14 примеров с 4 атрибутами для предметной области "Выбор заказчика"
   - Реализовать алгоритм ID3 (Information Gain)
   - Реализовать алгоритм C4.5 (Gain Ratio)
   - Реализовать алгоритм CART (Gini Index)
   - Реализовать алгоритм CHAID (Chi-Square)

3. **Анализ и сравнение:**
   - Построить деревья решений для всех алгоритмов
   - Визуализировать полученные деревья
   - Сравнить результаты работы алгоритмов
   - Выявить различия в структуре деревьев и эффективности алгоритмов

---

## 2. ХОД РАБОТЫ

### 2.1. Определение предметной области

Была выбрана предметная область **"Выбор заказчика"** (вариант 13 согласно заданию). Определены следующие атрибуты для принятия решения:

**Входные атрибуты:**
1. **Бюджет** — финансовые возможности заказчика (значения: Низкий, Средний, Высокий)
2. **Надежность** — репутация и надежность заказчика (значения: Низкая, Средняя, Высокая)
3. **Опыт совместной работы** — наличие предыдущего опыта сотрудничества (значения: Нет, Есть)
4. **Срок оплаты** — условия оплаты работ (значения: Короткий, Длинный)

**Целевой атрибут:**
- **Выбрать** — решение о сотрудничестве с заказчиком (значения: Да, Нет)

### 2.2. Создание обучающей выборки

Была создана обучающая выборка из 14 примеров. Данные подбирались таким образом, чтобы:
- Все 4 атрибута влияли на принятие решения
- Деревья имели достаточную глубину для демонстрации работы алгоритмов
- Присутствовали неоднозначные ситуации, требующие анализа нескольких атрибутов

Пример обучающей выборки (первые 5 записей):

| № | Бюджет | Надежность | Опыт | Срок оплаты | Решение |
|---|--------|------------|------|-------------|---------|
| 1 | Высокий | Высокая | Есть | Короткий | Да |
| 2 | Высокий | Средняя | Есть | Короткий | Да |
| 3 | Средний | Высокая | Есть | Короткий | Да |
| 4 | Высокий | Высокая | Нет | Короткий | Нет |
| 5 | Низкий | Высокая | Есть | Короткий | Нет |

### 2.3. Проектирование структуры программы

Проект был организован по модульному принципу без использования объектно-ориентированного программирования (согласно требованиям):

**Структура файлов:**
```
├── data.h          # Обучающая выборка
├── entropy.h       # Функции расчета метрик
├── id3.h           # Алгоритм ID3
├── c45.h           # Алгоритм C4.5
├── cart.h          # Алгоритм CART
├── chaid.h         # Алгоритм CHAID
├── main.cpp        # Главная программа
├── Makefile        # Система сборки
└── README.md       # Документация
```

### 2.4. Реализация вспомогательных функций

В файле `entropy.h` были реализованы ключевые функции:

**1. Расчет энтропии:**
```cpp
double calculate_entropy(const std::vector<Example>& data) {
    // E(S) = -Σ(pi * log2(pi))
    // где pi - доля примеров класса i
}
```

**2. Расчет прироста информации (для ID3):**
```cpp
double calculate_information_gain(const std::vector<Example>& data, int attr_index) {
    // IG(S, A) = E(S) - Σ(|Sv|/|S| * E(Sv))
}
```

**3. Расчет Gain Ratio (для C4.5):**
```cpp
double calculate_gain_ratio(const std::vector<Example>& data, int attr_index) {
    // GainRatio(S, A) = IG(S, A) / SplitInfo(S, A)
}
```

**4. Расчет индекса Джини (для CART):**
```cpp
double calculate_gini_index(const std::vector<Example>& data) {
    // Gini(S) = 1 - Σ(pi²)
}
```

**5. Расчет критерия хи-квадрат (для CHAID):**
```cpp
double calculate_chi_square(const std::vector<Example>& data, int attr_index) {
    // χ² = Σ((O - E)² / E)
}
```

### 2.5. Реализация алгоритма ID3

Алгоритм ID3 реализован в файле `id3.h`. Основные этапы:

1. **Базовые случаи:**
   - Если все примеры относятся к одному классу → создать лист с этим классом
   - Если атрибуты закончились → создать лист с наиболее частым классом

2. **Выбор атрибута:**
   - Для каждого доступного атрибута вычислить прирост информации
   - Выбрать атрибут с максимальным IG

3. **Рекурсивное построение:**
   - Создать узел с выбранным атрибутом
   - Разбить данные по значениям атрибута
   - Рекурсивно построить поддеревья

### 2.6. Реализация алгоритма C4.5

Алгоритм C4.5 (файл `c45.h`) отличается от ID3 использованием **Gain Ratio** вместо Information Gain. Это устраняет смещение к атрибутам с большим количеством значений за счет нормализации через Split Information.

### 2.7. Реализация алгоритма CART

Алгоритм CART (файл `cart.h`) использует **индекс Джини** вместо энтропии. Индекс Джини вычислительно более эффективен, так как не требует вычисления логарифмов.

### 2.8. Реализация алгоритма CHAID

Алгоритм CHAID (файл `chaid.h`) использует статистический критерий **хи-квадрат** для оценки значимости связи между атрибутом и целевой переменной.

### 2.9. Создание главной программы

В файле `main.cpp` реализована программа, которая:
1. Загружает обучающую выборку
2. Выводит данные в табличном формате
3. Последовательно строит деревья всеми алгоритмами
4. Визуализирует деревья в текстовом виде
5. Выводит характеристики (глубину, количество узлов)
6. Создает сравнительную таблицу

### 2.10. Создание системы сборки

Создан `Makefile` с командами:
- `make` — компиляция проекта
- `make run` — сборка и запуск
- `make clean` — очистка объектных файлов
- `make rebuild` — пересборка с нуля

---

## 3. РЕЗУЛЬТАТЫ РАБОТЫ

### 3.1. Построенные деревья решений

Все четыре алгоритма успешно построили деревья решений. Результаты:

**Сравнительная таблица:**
```
┌──────────────┬──────────┬────────────────┬─────────────────────────┐
│  Алгоритм    │ Глубина  │ Кол-во узлов   │      Критерий           │
├──────────────┼──────────┼────────────────┼─────────────────────────┤
│ ID3          │    3     │       9        │ Information Gain        │
│ C4.5         │    3     │       8        │ Gain Ratio              │
│ CART         │    3     │       9        │ Gini Index              │
│ CHAID        │    3     │       8        │ Chi-Square              │
└──────────────┴──────────┴────────────────┴─────────────────────────┘
```

### 3.2. Дерево ID3

```
└── Опыт
    ├── [Есть]
    │   └── Бюджет
    │       ├── [Высокий]
    │       │   └── Надежность
    │       │       ├── [Высокая] → Да
    │       │       ├── [Низкая] → Нет
    │       │       └── [Средняя] → Да
    │       ├── [Низкий] → Нет
    │       └── [Средний] → Да
    └── [Нет] → Нет
```

**Интерпретация:** ID3 определил "Опыт совместной работы" как наиболее важный атрибут. Если опыта нет, заказчика не выбирают. При наличии опыта решение зависит от бюджета и надежности.

### 3.3. Дерево C4.5

```
└── Опыт
    ├── [Есть]
    │   └── Срок_оплаты
    │       ├── [Длинный] → Нет
    │       └── [Короткий]
    │           └── Бюджет
    │               ├── [Высокий] → Да
    │               ├── [Низкий] → Нет
    │               └── [Средний] → Да
    └── [Нет] → Нет
```

**Интерпретация:** C4.5 также выбрал "Опыт" как корневой атрибут, но на втором уровне использовал "Срок оплаты" вместо "Бюджет". Дерево получилось более компактным (8 узлов против 9).

### 3.4. Дерево CART

Дерево CART идентично ID3 по структуре (9 узлов), но использует индекс Джини вместо энтропии для выбора атрибутов.

### 3.5. Дерево CHAID

```
└── Опыт
    ├── [Есть]
    │   └── Бюджет
    │       ├── [Высокий]
    │       │   └── Срок_оплаты
    │       │       ├── [Длинный] → Нет
    │       │       └── [Короткий] → Да
    │       ├── [Низкий] → Нет
    │       └── [Средний] → Да
    └── [Нет] → Нет
```

**Интерпретация:** CHAID построил компактное дерево (8 узлов), используя статистический критерий хи-квадрат.

### 3.6. Анализ различий

**Ключевые наблюдения:**

1. **Все алгоритмы** выбрали атрибут **"Опыт"** как корневой — это означает, что он наиболее информативен для принятия решения.

2. **C4.5 и CHAID** построили более компактные деревья (8 узлов) по сравнению с ID3 и CART (9 узлов). Это демонстрирует эффективность:
   - Нормализации через Gain Ratio в C4.5
   - Статистического подхода в CHAID

3. **Порядок атрибутов** различается:
   - ID3, CART: Опыт → Бюджет → Надежность
   - C4.5: Опыт → Срок_оплаты → Бюджет
   - CHAID: Опыт → Бюджет → Срок_оплаты

4. **Использование атрибутов:**
   - Все алгоритмы используют 3 атрибута (Опыт, Бюджет, и Надежность или Срок_оплаты)
   - Атрибут "Опыт" оказался критическим для всех алгоритмов

### 3.7. Извлеченные правила принятия решений

Из деревьев можно извлечь следующие бизнес-правила:

**Выбрать заказчика (Да):**
- Если есть опыт совместной работы И бюджет средний или высокий (с некоторыми исключениями по надежности и срокам оплаты)

**Не выбирать (Нет):**
- Если нет опыта совместной работы (всегда)
- Если опыт есть, но бюджет низкий
- Если опыт есть, бюджет высокий, но срок оплаты длинный (по некоторым алгоритмам)

---

## 4. ПРОБЛЕМЫ И ПОМОЩЬ НЕЙРОСЕТИ

В процессе выполнения лабораторной работы возникло несколько сложностей, в решении которых помогла нейросеть (AI-ассистент).

### 4.1. Проблема: Недостаточная глубина деревьев

**Описание проблемы:**
При первоначальной разработке обучающей выборки все четыре алгоритма строили очень простые деревья глубиной всего 1 уровень. Дерево выглядело так:
```
└── Срок_оплаты
    ├── [Длинный] → Нет
    └── [Короткий] → Да
```

Атрибут "Срок_оплаты" идеально разделял все примеры на два класса, после чего энтропия становилась равной 0 и дальнейшее разбиение не требовалось. Это делало лабораторную работу неинформативной, так как:
- Не демонстрировалась работа алгоритмов на глубоких уровнях
- Использовался только 1 из 4 атрибутов
- Не было видно различий между алгоритмами

**Решение с помощью нейросети:**
Нейросеть предложила изменить обучающую выборку, создав неоднозначные ситуации, где один атрибут не мог полностью определить решение. Были внесены изменения в данные:
- Пример 6: изменен результат с "Да" на "Нет" при средних параметрах
- Пример 11: изменен срок оплаты для создания зависимости от других атрибутов
- Пример 13-14: скорректированы для усложнения логики

После изменений деревья стали глубиной 3 уровня и использовали 3 атрибута.

### 4.2. Проблема: Неиспользование атрибута "Опыт"

**Описание проблемы:**
При промежуточном варианте данных деревья использовали только атрибуты "Срок_оплаты", "Бюджет" и "Надежность", но не использовали "Опыт совместной работы". Это не соответствовало требованию задания о демонстрации работы всех 4 атрибутов.

**Решение с помощью нейросети:**
Нейросеть объяснила, что для включения атрибута "Опыт" в дерево необходимо создать ситуации, где при одинаковых других параметрах наличие или отсутствие опыта меняет решение. Был изменен пример 4:
- Было: `{Высокий, Высокая, Нет, Короткий, Да}`
- Стало: `{Высокий, Высокая, Нет, Короткий, Нет}`

Это создало зависимость решения от опыта, и атрибут "Опыт" стал корневым во всех деревьях, что логично соответствует бизнес-логике — опыт совместной работы действительно критически важен.

### 4.3. Проблема: Понимание различий между алгоритмами

**Описание проблемы:**
Изначально было сложно понять теоретические различия между алгоритмами и почему они дают разные результаты при одних и тех же данных.

**Решение с помощью нейросети:**
Нейросеть предоставила подробные объяснения:

1. **ID3 vs C4.5:** Объяснила, что ID3 имеет смещение (bias) к атрибутам с большим числом значений, а C4.5 устраняет это через нормализацию (Gain Ratio = IG / SplitInfo).

2. **Энтропия vs Gini:** Разъяснила, что индекс Джини вычислительно эффективнее энтропии (не требует логарифмов), но оба метода основаны на измерении неоднородности данных.

3. **CHAID:** Объяснила статистический подход с использованием критерия хи-квадрат, который проверяет значимость связи между атрибутами и классом.

### 4.4. Проблема: Реализация функций расчета метрик

**Описание проблемы:**
Возникли сложности с реализацией математических формул, особенно:
- Учет случая, когда вероятность равна 0 (log₂(0) не определен)
- Правильное вычисление взвешенной суммы
- Расчет ожидаемых частот для критерия хи-квадрат

**Решение с помощью нейросети:**
Нейросеть предоставила корректные реализации с обработкой граничных случаев:

```cpp
// Правильная обработка log2(0)
if (probability > 0) {
    entropy -= probability * log2(probability);
}

// Корректный расчет ожидаемой частоты для χ²
double expected = (double)subset_size * class_pair.second / total;
if (expected > 0) {
    double diff = observed - expected;
    chi_square += (diff * diff) / expected;
}
```

### 4.5. Проблема: Структура проекта

**Описание проблемы:**
Было неясно, как организовать код без использования ООП, сохраняя при этом читаемость и модульность.

**Решение с помощью нейросети:**
Нейросеть предложила модульную структуру на основе заголовочных файлов (header-only подход):
- Разделение данных, вычислительных функций и алгоритмов
- Использование структур вместо классов
- Функциональный подход с передачей данных через параметры

Это позволило создать чистую и понятную архитектуру без ООП.

### 4.6. Проблема: Визуализация деревьев

**Описание проблемы:**
Требовалось создать понятную текстовую визуализацию деревьев с отображением иерархии.

**Решение с помощью нейросети:**
Нейросеть реализовала рекурсивную функцию `print_tree()` с использованием псевдографики:
```
└── Опыт
    ├── [Есть]
    │   └── Бюджет
    │       ├── [Высокий] → Да
```

Функция корректно отображает вложенность, значения атрибутов и листовые узлы с классами.

---

## 5. ВЫВОДЫ

### 5.1. Основные результаты

В ходе выполнения лабораторной работы были решены все поставленные задачи:

1. ✅ **Изучена теория** деревьев решений и математические основы четырех алгоритмов
2. ✅ **Реализованы алгоритмы** ID3, C4.5, CART и CHAID на языке C++
3. ✅ **Создана обучающая выборка** из 14 примеров с 4 атрибутами
4. ✅ **Построены деревья решений** для предметной области "Выбор заказчика"
5. ✅ **Проведен сравнительный анализ** алгоритмов
6. ✅ **Документирован код** с комментариями на русском языке

### 5.2. Теоретические выводы

**Различия алгоритмов:**

1. **ID3** — базовый алгоритм, простой и понятный, но имеет смещение к атрибутам с большим количеством значений.

2. **C4.5** — улучшенная версия ID3 с нормализацией через Gain Ratio. Строит более компактные и сбалансированные деревья. В нашем эксперименте показал лучший результат (8 узлов).

3. **CART** — использует индекс Джини, что вычислительно эффективнее. Хорошо подходит для бинарных разбиений и задач регрессии.

4. **CHAID** — статистический подход с критерием хи-квадрат. Учитывает статистическую значимость разбиений. Также показал компактное дерево (8 узлов).

**Важность выбора атрибутов:**
Все алгоритмы согласованно выбрали "Опыт совместной работы" как наиболее важный атрибут. Это подтверждает корректность работы алгоритмов и адекватность обучающей выборки.

### 5.3. Практические выводы

**Применимость алгоритмов:**
- Для задач с категориальными данными подходят все четыре алгоритма
- C4.5 и CHAID показали лучшую оптимизацию структуры дерева
- ID3 хорош для обучения и понимания базовых принципов
- CART эффективен при необходимости быстрых вычислений

**Качество данных:**
Качество обучающей выборки критически важно для построения информативных деревьев. Данные должны:
- Содержать неоднозначные случаи
- Демонстрировать зависимости от различных атрибутов
- Быть сбалансированными по классам

### 5.4. Образовательные результаты

Выполнение лабораторной работы позволило:

1. **Глубоко понять** математические основы деревьев решений (энтропия, прирост информации, индекс Джини, критерий хи-квадрат)

2. **Получить практический опыт** реализации алгоритмов машинного обучения на языке C++

3. **Научиться сравнивать** различные подходы к решению одной задачи

4. **Развить навыки** работы с AI-ассистентом для решения технических проблем

### 5.5. Роль нейросети в обучении

Использование AI-ассистента (нейросети) в процессе выполнения работы показало себя эффективным инструментом обучения:

**Положительные аспекты:**
- Быстрое получение объяснений сложных концепций
- Помощь в отладке и поиске ошибок
- Предложения по улучшению структуры кода
- Генерация примеров и тестовых данных

**Сохранение учебной ценности:**
При этом все ключевые решения принимались самостоятельно, нейросеть выступала в роли опытного консультанта, а не заменяла процесс обучения.

### 5.6. Возможные направления развития

**Для углубления знаний можно:**
1. Реализовать визуализацию деревьев в графическом формате
2. Добавить функционал предсказания класса для новых примеров
3. Реализовать оценку качества деревьев (точность, precision, recall)
4. Добавить кросс-валидацию для оценки обобщающей способности
5. Реализовать pruning (обрезку) деревьев для предотвращения переобучения
6. Добавить поддержку непрерывных атрибутов

---

## ЗАКЛЮЧЕНИЕ

Лабораторная работа успешно выполнена. Реализованы четыре алгоритма построения деревьев решений, проведен их сравнительный анализ. Полученные результаты подтверждают теоретические знания о различиях между алгоритмами ID3, C4.5, CART и CHAID.

Практическая реализация позволила глубже понять принципы работы деревьев решений и сформировать навыки разработки систем машинного обучения. Использование AI-ассистента продемонстрировало эффективность современных инструментов в образовательном процессе при сохранении критического мышления и самостоятельного принятия решений.

---

**Дата выполнения:** 4 декабря 2025 г.
